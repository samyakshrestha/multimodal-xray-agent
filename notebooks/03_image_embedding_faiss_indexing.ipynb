{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNJAHQn4ppUwPaV3t1luflM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Notebook: CheXpert + ChestXray14 Preprocessing and Image Embedding\n","\n","This notebook performs all image-side preprocessing and indexing for the multimodal chest X-ray retrieval pipeline. It prepares embeddings for over 350,000 X-ray images across two datasets—**CheXpert** and **ChestXray14**—and saves them in a FAISS index for fast nearest-neighbor retrieval. The notebook runs entirely on the Colab SSD using an A100 GPU to avoid Google Drive I/O limitations.\n","\n","\n","---\n","\n","\n","## Background: What Are Embeddings and FAISS?\n","\n","### What is an Image Embedding?\n","An **embedding** is a dense vector representation of an image (or text) that captures its semantic meaning in a high-dimensional space. In this notebook, we use a pretrained **BiomedCLIP** vision encoder to convert each chest X-ray image into a 512-dimensional vector that encodes its visual features in a way that is meaningful for similarity comparisons.\n","\n","These vectors allow us to compare images not by raw pixel values, but by how semantically similar they are—e.g., images with similar medical findings are mapped closer together.\n","\n","### What is FAISS?\n","**FAISS** (Facebook AI Similarity Search) is a library for fast, scalable nearest-neighbor search on dense vectors. It supports indexing millions of high-dimensional embeddings and querying them efficiently using various distance metrics like cosine similarity or Euclidean distance.\n","\n","In this project, we:\n","- Normalize each image embedding to unit length\n","- Use **FAISS's IndexFlatIP** (inner product) to perform similarity search over the normalized vectors\n","- Store the index and aligned UUIDs for real-time retrieval in the Radiology Assistant\n","\n","This enables us to later retrieve the most visually similar medical images to a user-uploaded query in real-time, without needing to store or access the full image dataset during inference.\n","\n","---\n","\n","## Workflow Overview\n","\n","### **Step 1 – Environment Setup**\n","- Verified GPU (A100 or T4) and selected appropriate compute backend (`torch.device`).\n","- Installed required libraries (e.g., `open_clip_torch`, `faiss-cpu`).\n","\n","### **Step 2 – Load Preprocessed CheXpert Data**\n","- Copied and unzipped `chexpert_flat.zip` into `/content/chexpert/` on Colab SSD.\n","- All images were converted to 224×224 grayscale `.png` in a prior preprocessing stage.\n","\n","### **Step 3 – Define Paths**\n","- Defined fixed path constants for:\n","  - CheXpert directory: `/content/chexpert/`\n","  - Chest14 directory: `/content/images-224/images-224/`\n","  - Drive-relative paths for UUID mapping and FAISS save locations.\n","\n","### **Step 4 – Define Transforms**\n","- Used OpenCLIP's BiomedCLIP transform: `Resize(224) → CenterCrop → ToTensor → Normalize(...)` from the pretrained preprocessing pipeline.\n","\n","### **Step 5 – Launch Preprocessing (CheXpert Only)**\n","- Preprocessing was already complete; this step involved verifying and preparing standardized filenames.\n","- No new image files were created—just reused existing `.png`s on SSD.\n","\n","### **Step 6 – Generate Image Metadata**\n","- Created `image_metadata` list of dictionaries with:\n","  - `uuid`: unique image ID\n","  - `path`: relative Drive path for later lookup\n","  - `dataset`: either `\"chexpert\"` or `\"chest14\"`\n","\n","### **Step 7 – Save Manifest**\n","- Dumped `image_metadata.jsonl` (≈45 MB) using `json.dumps` line-by-line format.\n","- Manifest persisted to Drive: `./data/indexes/image_metadata.jsonl`\n","\n","### **Step 8 – Load BiomedCLIP Vision Encoder**\n","- Loaded `microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224` using `open_clip.create_model_from_pretrained(...)`.\n","- Only `encode_image()` was used; model weights remained frozen.\n","\n","### **Step 9 – Embed Images in Batches**\n","- Iterated over all entries in `image_metadata.jsonl`.\n","- Loaded images from **SSD**, preprocessed, and passed through BiomedCLIP.\n","- Appended resulting 512-D float32 embeddings and UUIDs to memory.\n","\n","### **Step 10 – Build FAISS Index**\n","- Normalized embeddings with `L2 norm` for cosine similarity.\n","- Used `faiss.IndexFlatIP(dim=512)` and wrote the binary index to:\n","  - `./data/indexes/image_faiss.bin`\n","  - `./data/indexes/image_uuids.json`\n","\n","---\n","\n","## Final Output Files\n","\n","```\n","| Filename                  | Purpose                                   | Size     |\n","|---------------------------|--------------------------------------------|----------|\n","| `image_faiss.bin`         | FAISS index for ~350k images               | ~655 MB  |\n","| `image_uuids.json`        | One-to-one mapping from index to UUID      | ~12 MB   |\n","| `image_metadata.jsonl`    | Full manifest with path + dataset info     | ~45 MB   |\n","\n","```"],"metadata":{"id":"RGyIEC2FAFvA"}},{"cell_type":"markdown","source":["## Step 0: Mounting Google Drive and Importing Libraries"],"metadata":{"id":"W-lfhM0dDkuW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrJn5lsaDeNc"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/multimodal-xray-agent\n","!ls"]},{"cell_type":"code","source":["!pip install open_clip_torch faiss-cpu -q"],"metadata":{"id":"QRi8nG-CYukE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import faiss\n","import numpy as np\n","import glob, random\n","import itertools, pprintimport os, uuid, json, torch, shutil\n","\n","from tqdm import tqdm\n","from PIL import Image\n","from pathlib import Path\n","from torchvision import transforms\n","from concurrent.futures import ThreadPoolExecutor\n","from open_clip import create_model_from_pretrained, get_tokenizer\n","\n","from src.chexpert_preprocessing import process_one"],"metadata":{"id":"XS2iiuxLDrg-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 1: Verifying GPU and Environment"],"metadata":{"id":"uFk_DUroDx7o"}},{"cell_type":"code","source":["# Device-agnostic setup\n","if torch.cuda.is_available():\n","    device_name = torch.cuda.get_device_name(0)\n","    device = torch.device(\"cuda\")\n","    print(f\"GPU detected: {device_name}\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not detected. Falling back to CPU.\")\n","\n","print(f\"Running on device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5UgRvGe5DykL","executionInfo":{"status":"ok","timestamp":1748511679765,"user_tz":300,"elapsed":10,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"b2dc284a-737a-43b8-8b97-222a4e2eafb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU detected: NVIDIA A100-SXM4-40GB\n","Running on device: cuda\n"]}]},{"cell_type":"markdown","source":["## Step 2: Loading CheXpert Data to Local SSD"],"metadata":{"id":"3tZOBi3WD15R"}},{"cell_type":"code","source":["!cp \"./data/images_sample/chexpert_flat.zip\" /content/"],"metadata":{"id":"p7Wvu-eoD53K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -q /content/chexpert_flat.zip -d /content"],"metadata":{"id":"-N0SbHdND6tH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!find /content/chexpert_flat -type f | head -n 3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_pa4Wc6D8iB","executionInfo":{"status":"ok","timestamp":1748508888249,"user_tz":300,"elapsed":204,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"a6916e68-4c38-4eeb-d789-10f97d7f195c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/chexpert_flat/patient36133_study1_view1_frontal.jpg\n","/content/chexpert_flat/patient24375_study8_view2_lateral.jpg\n","/content/chexpert_flat/patient28388_study17_view2_frontal.jpg\n"]}]},{"cell_type":"markdown","source":["## Step 3: Defining Paths"],"metadata":{"id":"Pf6utKfWEGB7"}},{"cell_type":"code","source":["IN_DIR  = Path(\"/content/chexpert_flat\").resolve()\n","OUT_DIR = Path(\"/content/chexpert\").resolve()\n","OUT_DIR.mkdir(parents=True, exist_ok=True)"],"metadata":{"id":"dfVe6WVTEGj1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: Define Transforms"],"metadata":{"id":"6GHa6YyyENlr"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),          # [0,1]  float32\n","])"],"metadata":{"id":"flPrEZGUEP88"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 5: Launch Parallel Processing (Preprocessing)\n","\n","This is essentially the same step that we used in our notebook to preprocess our dataset. The reason why we are repeating this step here again is that we want to load the files from Colab's SSD rather than Google Drive (which causes I/O errors due to the huge volume of the dataset). I am sure there are other, more optimal ways to solve this problem, but this is the best approach I could think of given my limited time."],"metadata":{"id":"kUhnD9_NEUBK"}},{"cell_type":"code","source":["image_paths = list(IN_DIR.glob(\"*.jpg\"))\n","print(f\"Discovered {len(image_paths):,} images\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fACtMLNnN95o","executionInfo":{"status":"ok","timestamp":1748509215168,"user_tz":300,"elapsed":1394,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"f99f4910-e905-4674-9ff8-5402a12477d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Discovered 223,416 images\n"]}]},{"cell_type":"code","source":["with ThreadPoolExecutor(max_workers=4) as pool:\n","    results = list(tqdm(pool.map(process_one, image_paths), total=len(image_paths)))\n","\n","print(f\"\\n Preprocessed {sum(results):,} / {len(image_paths):,} images into {OUT_DIR}\")"],"metadata":{"id":"RXaRCXObEWmb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 6: Generate Image Metadata"],"metadata":{"id":"6kNjGmTzFQB5"}},{"cell_type":"code","source":["image_metadata = []\n","\n","for fname in tqdm(os.listdir(OUT_DIR), desc=\"Build Chexpert image metadata from SSD\"):\n","    if fname.endswith(\".png\"):\n","        image_metadata.append({\n","            \"uuid\": str(uuid.uuid4()),\n","            \"path\": f\"data/images_sample/chexpert/{fname}\",  # <- POINTS TO DRIVE, NOT SSD\n","            \"dataset\": \"chexpert\"\n","        })"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kE8DsYuLFTGb","executionInfo":{"status":"ok","timestamp":1748509818433,"user_tz":300,"elapsed":969,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"c8adea39-5e2c-4f52-de0c-c83157b43c81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Build Chexpert image metadata from SSD: 100%|██████████| 223414/223414 [00:00<00:00, 263144.35it/s]\n"]}]},{"cell_type":"code","source":["# Now doing the same for the Chest14 dataset\n","!cp \"./data/images_sample/chest14.zip\" /content/"],"metadata":{"id":"gBMN69bDFYFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -q /content/chest14.zip -d /content"],"metadata":{"id":"BP_K33TuJvcz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!find /content/images-224 -type f | head -n 3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeEiiF0nJ3mz","executionInfo":{"status":"ok","timestamp":1748509896959,"user_tz":300,"elapsed":106,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"f125a420-49cc-4810-882d-76b5f02f501d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/images-224/images-224/00000001_000.png\n","/content/images-224/images-224/00000001_001.png\n","/content/images-224/images-224/00000001_002.png\n"]}]},{"cell_type":"code","source":["CHEST14_DIR = Path(\"/content/images-224/images-224\").resolve()\n","\n","for fname in tqdm(os.listdir(CHEST14_DIR), desc=\"Build Chest14 image metadata\"):\n","    if fname.endswith(\".png\"):\n","        image_metadata.append({\n","            \"uuid\": str(uuid.uuid4()),\n","            \"path\": f\"data/images_sample/chest14/{fname}\",  # <- POINTS TO DRIVE, NOT SSD\n","            \"dataset\": \"chest14\"\n","        })"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fX7xGFwEJ_27","executionInfo":{"status":"ok","timestamp":1748509913995,"user_tz":300,"elapsed":477,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"756c50f5-6de2-4395-a662-db1bf317bff8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Build Chest14 image metadata: 100%|██████████| 112120/112120 [00:00<00:00, 271400.47it/s]\n"]}]},{"cell_type":"markdown","source":["## Step 7: Saving the Metadata File as jsonl"],"metadata":{"id":"UtQVDV7gLzts"}},{"cell_type":"code","source":["# Save locally and copy to Drive\n","jsonl_path = \"/content/image_metadata.jsonl\"\n","with open(jsonl_path, \"w\") as f:\n","    for entry in image_metadata:\n","        f.write(json.dumps(entry) + \"\\n\")"],"metadata":{"id":"rLb-FI_MLxDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp /content/image_metadata.jsonl /content/drive/MyDrive/multimodal-xray-agent/data/indexes/image_metadata.jsonl"],"metadata":{"id":"J5mhmBYaMLdd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 8 – Load Vision Encoder: BiomedCLIP (OpenCLIP ViT-B/16)\n","\n","We load `microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224`, a domain-specific vision-language model pretrained on MIMIC-CXR and PMC figures. The vision backbone is a ViT-B/16 transformer, accessed via the `open_clip_torch` interface.\n","\n","**Why this model:**\n","- **Radiology-tuned**: Unlike generic CLIP, BiomedCLIP has seen large volumes of chest X-rays and biomedical image-text pairs.\n","- **Pretrained ViT encoder**: Delivers strong performance with no additional fine-tuning required.\n","- **Frozen weights**: The model is used only for inference (`eval()` mode), ensuring stable and reproducible feature extraction.\n","\n","**Purpose in pipeline:**\n","- Converts each 224×224 grayscale X-ray image into a 512-dimensional float32 embedding using `.encode_image(...)`.\n","- These embeddings are later indexed with FAISS for similarity-based image retrieval.\n","\n","The `preprocess` transform returned by the model includes resizing, normalization, and tensor conversion, ensuring input compatibility with the pretrained ViT-B/16 backbone."],"metadata":{"id":"2kySpw0maIG0"}},{"cell_type":"code","source":["ROOT_DIR = \"/content/drive/MyDrive/multimodal-xray-agent\"\n","IMG_DIR_CHEXPERT = os.path.join(ROOT_DIR, \"data/images_sample/chexpert\")\n","IMG_DIR_CHEST14 = os.path.join(ROOT_DIR, \"data/images_sample/chest14\")\n","INDEX_OUT_DIR = os.path.join(ROOT_DIR, \"data/indexes\")\n","META_OUT_PATH = os.path.join(ROOT_DIR, \"data/indexes/image_metadata.jsonl\") # This is where the image metadata is stored\n","\n","os.makedirs(INDEX_OUT_DIR, exist_ok=True)"],"metadata":{"id":"vBkJJqDcV4E0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"ViT-B-16\"\n","hf_repo = \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\""],"metadata":{"id":"tkgxrbEAaTC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model and preprocessing from Hugging Face Hub\n","model, preprocess = create_model_from_pretrained(\n","    \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",")"],"metadata":{"id":"SQX_yvevaUgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = model.to(device).eval()"],"metadata":{"id":"fehRCGtUaV-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Output shape (dummy):\", model.encode_image(preprocess(Image.new(\"RGB\", (224, 224))).unsqueeze(0).to(device)).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIIa0w9QaZW0","executionInfo":{"status":"ok","timestamp":1748511695514,"user_tz":300,"elapsed":363,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"88a0f959-45ca-4793-fc6e-15c4590ee6f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape (dummy): torch.Size([1, 512])\n"]}]},{"cell_type":"markdown","source":["## Step 9 – Embed All Images into Vector Space\n","\n","This step encodes each preprocessed chest X-ray into a fixed-length vector using the BiomedCLIP vision transformer.\n","\n","**Process:**\n","- Loads `image_metadata.jsonl` from local SSD, which contains a UUID and path for each image.\n","- Depending on the dataset (`chexpert` or `chest14`), constructs the correct absolute file path on SSD.\n","- Each image is:\n","  1. Loaded via PIL and converted to RGB (as expected by ViT).\n","  2. Preprocessed using BiomedCLIP’s `preprocess(...)` transform (resize, normalize, tensorize).\n","  3. Passed to `model.encode_image(...)` under `torch.no_grad()` to generate a 512-dimensional float32 embedding.\n","\n","**Output:**\n","- `all_embeddings`: a list of NumPy arrays (shape: 512-d per image).\n","- `all_uuids`: a parallel list of UUIDs for indexing and retrieval linkage.\n"],"metadata":{"id":"CKsmT7m6cmNZ"}},{"cell_type":"code","source":["# Load manifest from SSD (not Drive)\n","with open(\"/content/image_metadata.jsonl\", \"r\") as f:\n","    image_metadata = [json.loads(line) for line in f]"],"metadata":{"id":"zjvcvchEbzKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_embeddings = []\n","all_uuids = []\n","\n","for entry in tqdm(image_metadata, desc=\"Embedding images from SSD\"):\n","    fname = Path(entry[\"path\"]).name\n","\n","    if entry[\"dataset\"] == \"chexpert\":\n","        actual_path = Path(\"/content/chexpert\") / fname\n","    elif entry[\"dataset\"] == \"chest14\":\n","        actual_path = Path(\"/content/images-224/images-224\") / fname\n","    else:\n","        continue\n","\n","    try:\n","        img = Image.open(actual_path).convert(\"RGB\")\n","        img_tensor = preprocess(img).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            emb = model.encode_image(img_tensor).cpu().numpy()\n","        all_embeddings.append(emb)\n","        all_uuids.append(entry[\"uuid\"])\n","    except Exception as e:\n","        print(f\"[ERROR] {actual_path}: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TT2q2N-Jdfg8","executionInfo":{"status":"ok","timestamp":1748516977947,"user_tz":300,"elapsed":2679351,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"23b09365-ae5b-4d52-bf42-cdd499a358a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Embedding images from SSD: 100%|██████████| 335534/335534 [44:39<00:00, 125.23it/s]\n"]}]},{"cell_type":"markdown","source":["## Step 10 – Build FAISS Index & Persist Outputs\n","\n","This step constructs a high-performance image similarity index using FAISS to enable cosine-based nearest neighbor search over the full 350,000-image corpus.\n","\n","**Process:**\n","- **Embedding Flattening:** All 512-dimensional image vectors (`all_embeddings`) are vertically stacked into a single NumPy array of shape `(N, 512)`.\n","- **Normalization:** Each vector is L2-normalized so that cosine similarity reduces to inner product (dot product) in FAISS.\n","- **FAISS Index:** Uses `IndexFlatIP` to build an exact inner-product search index over the normalized vectors.\n","\n","**Storage:**\n","- The FAISS binary index is written to `image_faiss.bin` (≈ 650 MB).\n","- The corresponding `image_uuids.json` file stores the aligned UUIDs for post-retrieval lookup and captioning.\n","\n","**Why It Matters:**\n","- Enables efficient sub-50ms inference-time retrieval over hundreds of thousands of medical images.\n","- Index is self-contained: downstream modules (agents, FastAPI) use only this file + UUIDs for retrieval—raw images are not needed."],"metadata":{"id":"dE2wxO7jw830"}},{"cell_type":"code","source":["# Flatten and normalize embeddings\n","embeddings = np.vstack(all_embeddings).astype(\"float32\")\n","embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)  # L2-normalization for cosine similarity"],"metadata":{"id":"MnPMfVjHvUy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize FAISS index for Inner Product (cosine similarity via normalized vectors)\n","index = faiss.IndexFlatIP(embeddings.shape[1])\n","index.add(embeddings)"],"metadata":{"id":"NtdtRm1yx6mA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INDEX_OUT_DIR = os.path.join(ROOT_DIR, \"data/indexes\")\n","INDEX_PATH = os.path.join(INDEX_OUT_DIR, \"image_faiss.bin\")\n","UUIDS_PATH = os.path.join(INDEX_OUT_DIR, \"image_uuids.json\")\n","os.makedirs(INDEX_OUT_DIR, exist_ok=True)"],"metadata":{"id":"9TJlqCEix9z8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save index and UUIDs\n","faiss.write_index(index, INDEX_PATH)\n","with open(UUIDS_PATH, \"w\") as f:\n","    json.dump(all_uuids, f)\n","\n","print(f\"FAISS index saved to: {INDEX_PATH}\")\n","print(f\"UUID list saved to: {UUIDS_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gstpg20KycTU","executionInfo":{"status":"ok","timestamp":1748517950585,"user_tz":300,"elapsed":2356,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"0c3a88f1-9ea9-4bfe-c9a1-bd72eb8ee215"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FAISS index saved to: /content/drive/MyDrive/multimodal-xray-agent/data/indexes/image_faiss.bin\n","UUID list saved to: /content/drive/MyDrive/multimodal-xray-agent/data/indexes/image_uuids.json\n"]}]},{"cell_type":"code","source":["!tree /content/drive/MyDrive/multimodal-xray-agent -L 3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iM6EnxK_8eY_","executionInfo":{"status":"ok","timestamp":1748571295332,"user_tz":300,"elapsed":5615,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"28831a3b-db21-4ccc-a087-28ad43c62764"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01;34m/content/drive/MyDrive/multimodal-xray-agent\u001b[0m\n","├── \u001b[01;34mapp\u001b[0m\n","├── \u001b[01;31mchexpert.zip\u001b[0m\n","├── \u001b[01;34mdata\u001b[0m\n","│   ├── \u001b[01;34mimages_sample\u001b[0m\n","│   │   ├── \u001b[01;34mchest14\u001b[0m\n","│   │   ├── \u001b[01;31mchest14.zip\u001b[0m\n","│   │   ├── \u001b[01;34mchexpert\u001b[0m\n","│   │   └── \u001b[01;31mchexpert_flat.zip\u001b[0m\n","│   ├── \u001b[01;34mindexes\u001b[0m\n","│   │   ├── \u001b[00mimage_faiss.bin\u001b[0m\n","│   │   ├── \u001b[00mimage_metadata.jsonl\u001b[0m\n","│   │   └── \u001b[00mimage_uuids.json\u001b[0m\n","│   └── \u001b[01;34mqapairs\u001b[0m\n","├── \u001b[01;34mdeployment\u001b[0m\n","├── \u001b[00mLICENSE\u001b[0m\n","├── \u001b[01;34mmodels\u001b[0m\n","│   ├── \u001b[01;34mbiogpt_awq\u001b[0m\n","│   └── \u001b[01;34mlora_adapter\u001b[0m\n","├── \u001b[01;34mnotebooks\u001b[0m\n","│   ├── \u001b[00m00_colab_setup.ipynb\u001b[0m\n","│   ├── \u001b[00m01_bootstrap.ipynb\u001b[0m\n","│   ├── \u001b[00m02_preprocessing.ipynb\u001b[0m\n","│   └── \u001b[00m03_image_embedding_faiss_indexing.ipynb\u001b[0m\n","├── \u001b[00mPROJECT_LOG.md\u001b[0m\n","├── \u001b[00mREADME.md\u001b[0m\n","├── \u001b[00mrequirements.txt\u001b[0m\n","├── \u001b[01;34mscripts\u001b[0m\n","│   └── \u001b[00mpreprocess_chexpert_gpu.py\u001b[0m\n","└── \u001b[01;34msrc\u001b[0m\n","    ├── \u001b[00mchexpert_preprocessing.py\u001b[0m\n","    └── \u001b[01;34m__pycache__\u001b[0m\n","        └── \u001b[00mchexpert_preprocessing.cpython-311.pyc\u001b[0m\n","\n","15 directories, 17 files\n"]}]}]}