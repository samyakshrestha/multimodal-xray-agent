{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqyGGnFDiC0Q"
   },
   "source": [
    "## Step 0: Mounting Google Drive and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYDd4tPqMsEe"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/multimodal-xray-agent\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYuLkbPkWv1c"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSmuHDM1FtAk"
   },
   "outputs": [],
   "source": [
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNOKGGDXnyNT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.utils import logging as hf_logging\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, Dataset, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, get_peft_model_state_dict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, EvalPrediction, default_data_collator\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "623eba13c1a9493fab934b61885bc1b3",
      "b20731e5bbce490984fa9684563a0ae6",
      "faff6d216b5a4e0b9093e7567542dd1e",
      "f31353cd7c5945f78f3dfe2afce0e6a3",
      "6130c406aae54b6784eb9616119550c8",
      "4a2fb2316bec4aa49c7f9996eefeb0f0",
      "de79dd80d9d8406eb947e0be594262be",
      "a0526e10d1ed42a6bd5d56dd41a46f79",
      "2d2df58c358244fb8235b0c7a0c80109",
      "70f8ae346ed04b1dad32282abe7e8829",
      "a325797a7df34b93ae66c90386277861",
      "8a8be4f77235401493c24ebf960bfa3f",
      "633c2fd5952547e4a7b80ee631a7eb5f",
      "50cf71c256304e3ab1838343a5da92c7",
      "0749152d20474143a8ca7bddf727d020",
      "c45543485a62489a83c1e378bb22a182",
      "c844c47820904712956f5a01dded8531",
      "e820d5ba6f384591b4467c81baa15149",
      "ca3eb55948264774a1c5ddb194266022",
      "9b159fe7e9e04d4c9822048c20cebbf9"
     ]
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1749936852549,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "b_6GG_za-NCi",
    "outputId": "befa921d-425c-4fa3-9fc0-e4bfe9d23451"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623eba13c1a9493fab934b61885bc1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "camQQpz2kADJ"
   },
   "source": [
    "## Step 1: Verifying GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749936857028,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "y-YJrdRFiDws",
    "outputId": "e1b177bb-c31c-497b-bc05-d9b3b88e6984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: NVIDIA A100-SXM4-40GB\n",
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU detected: {device_name}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not detected. Falling back to CPU.\")\n",
    "\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raZi54636Cu3"
   },
   "source": [
    "## Step 2: Load & Preprocess Full Q/A Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D3UXbFKmKsU"
   },
   "outputs": [],
   "source": [
    "# Setting paths\n",
    "\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/multimodal-xray-agent\")\n",
    "QA_DIR = PROJECT_ROOT / \"data\" / \"qapairs\"\n",
    "ADAPTER_SAVE_PATH = PROJECT_ROOT / \"models\" / \"llama_lora_adapter\"\n",
    "OUTPUT_PATH = PROJECT_ROOT / \"data\" / \"qapairs\" / \"llama_validation_predictions.jsonl\"\n",
    "METRICS_PATH = PROJECT_ROOT / \"logs\" / \"llama_epoch_metrics.csv\"\n",
    "VAL_PATH = PROJECT_ROOT / \"data\" / \"qapairs\" / \"val.jsonl\"\n",
    "\n",
    "SOURCE_LOG_DIR = Path(\"./logs\")\n",
    "DEST_LOG_DIR = PROJECT_ROOT / \"logs\"\n",
    "\n",
    "DEST_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ADAPTER_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_PATH.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2205,
     "status": "ok",
     "timestamp": 1749936860705,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "0D-WH1qS63H5",
    "outputId": "4f333750-8348-4a93-9f49-86e667f2c805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 630\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 70\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(\"file://./data/tokenized_dataset\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1749936860768,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "42eBeEw-VtNI",
    "outputId": "f70bbc38-efdb-4361-e3a2-2cbb340ad6cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 975, 12044, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 861, 279, 3682, 73833, 94257, 14955, 304, 420, 2217, 13, 128009, 128006, 78191, 128007, 271, 2822, 30883, 73151, 454, 360, 55892, 1920, 26, 23900, 11, 4325, 18251, 65324, 296, 1123, 269, 94257, 67861, 42743, 64785, 79212, 488, 13, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2822, 30883, 73151, 454, 360, 55892, 1920, 26, 23900, 11, 4325, 18251, 65324, 296, 1123, 269, 94257, 67861, 42743, 64785, 79212, 488, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxJBnZbKRYMR"
   },
   "source": [
    "## Step 3: Model + Tokenizer Setup (QLoRA + FlashAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVmqY1t47Bdn"
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuaStPKa7R5d"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cOzNTZM7S84"
   },
   "outputs": [],
   "source": [
    "# It is crucial that the tokenizer here has the same pad_token setting\n",
    "# as the one used in the data preparation notebook.\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749936863820,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "1GWKj8OeTmYP",
    "outputId": "aeddf263-a2b0-4ab9-aed7-46013c931e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: <|eot_id|>\n",
      "Pad token ID: 128009\n",
      "EOS token: <|eot_id|>\n",
      "EOS token ID: 128009\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad token:\", tokenizer.pad_token)\n",
    "print(\"Pad token ID:\", tokenizer.pad_token_id)\n",
    "print(\"EOS token:\", tokenizer.eos_token)\n",
    "print(\"EOS token ID:\", tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_7qDKv_G4-t"
   },
   "outputs": [],
   "source": [
    "# Set quantization config for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4: best for LLMs\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VaI5DT5Rgmu"
   },
   "outputs": [],
   "source": [
    "# Load Llama model with FlashAttention if supported\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qe8qAkA6-Z6a"
   },
   "outputs": [],
   "source": [
    "base_model = prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749936910428,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "mTvSjcOTGyJT",
    "outputId": "29dc135f-6262-40df-ace7-9377a9b38189"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbHbO-wynqUl"
   },
   "source": [
    "## Step 4: LoRA Configuration + PEFT Wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5zqh3SCnoaS"
   },
   "outputs": [],
   "source": [
    "# Target modules for GPT2-style transformers (BioGPT)\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gEe4oQmWsLK"
   },
   "outputs": [],
   "source": [
    "# LoRA configuration (QLoRA-optimized)\n",
    "peft_config = LoraConfig(\n",
    "    r=64,                         # Rank of the LoRA decomposition\n",
    "    lora_alpha=128,                # Scaling factor\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,            # Regularization\n",
    "    bias=\"none\",                  # Do not fine-tune bias terms\n",
    "    task_type=\"CAUSAL_LM\",        # Language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQS5KUkBoxf1"
   },
   "outputs": [],
   "source": [
    "# Inject LoRA adapters into the base model\n",
    "model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsBfVUzBAIR-"
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-7AOh40AIQK"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1749936916062,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "1oR-oONqBksm",
    "outputId": "80c33931-022c-4f5e-e6f3-44bd14e0de90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 56,885,248 || all params: 3,269,635,072 || trainable%: 1.7398\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1749936917010,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "2IBHPhr8BxAm",
    "outputId": "8aec610e-0f31-493a-e018-6d5deb29cd8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-bmgRoHDz5F"
   },
   "source": [
    "## Step 5: TrainingArguments configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDfxdTM4DgCM"
   },
   "outputs": [],
   "source": [
    "# Define constants for clarity\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACC_STEPS = 4\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9H6RJDGtDjoI"
   },
   "outputs": [],
   "source": [
    "# Calculate total training steps\n",
    "total_training_samples = len(dataset[\"train\"])\n",
    "steps_per_epoch = ceil(total_training_samples / (BATCH_SIZE * GRAD_ACC_STEPS))\n",
    "total_steps = steps_per_epoch * EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749936919171,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "0lpy9ATyD17w",
    "outputId": "951b0ba0-92bd-4f1d-c944-4013dddde581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 40\n",
      "Warmup steps: 2\n"
     ]
    }
   ],
   "source": [
    "# Calculate warmup steps\n",
    "WARMUP_STEPS = int(0.05 * total_steps)\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {WARMUP_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhNDfFo0vSL3"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/lora_adapter\",                   # Save path\n",
    "    per_device_train_batch_size=BATCH_SIZE,               # Empirically stable for A100 with QLoRA\n",
    "    per_device_eval_batch_size=4,                         # Same for validation\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,           # Effective batch size = 12 × 2 = 24\n",
    "    eval_strategy=\"epoch\",                                # Evaluate once per epoch\n",
    "    save_strategy=\"epoch\",                                # Save checkpoint once per epoch\n",
    "    logging_strategy=\"steps\",                             # Log losses periodically\n",
    "    logging_dir=\"./logs\",                                 # Save logs\n",
    "    logging_steps=5,                                      # Log every 20 steps\n",
    "    num_train_epochs=EPOCHS,                              # Number of fine-tuning epochs\n",
    "    learning_rate=LEARNING_RATE,                          # Higher LR often better for small LoRA adapters\n",
    "    warmup_steps=WARMUP_STEPS,                            # Small warmup to stabilize first few updates\n",
    "    lr_scheduler_type=\"cosine\",                           # Smooth decay\n",
    "    save_total_limit=2,                                   # Retain 2 best checkpoints only\n",
    "    load_best_model_at_end=True,                          # Restore best checkpoint (lowest val loss)\n",
    "    report_to=\"tensorboard\",                              # Log to TensorBoard\n",
    "    run_name=\"llama-qlora-run\",                           # Appears in TensorBoard dashboard\n",
    "    bf16=True,\n",
    "    group_by_length=True,                                 # Efficient packing of similar-length samples\n",
    "    gradient_checkpointing=True,                          # Redundant with model setup, but safe to keep\n",
    "    eval_accumulation_steps=2,                            # Solves the CUDA OOM error during training\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bor_-l40QDKE"
   },
   "source": [
    "## Step 8: Adding Perplexity as an Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUE8N09nhIQc"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred: EvalPrediction) -> dict:\n",
    "    with torch.no_grad():\n",
    "        # Move the large tensors from GPU VRAM to system RAM (CPU)\n",
    "        # to prevent out-of-memory errors during metric calculation.\n",
    "        logits = torch.tensor(eval_pred.predictions).cpu()\n",
    "        labels = torch.tensor(eval_pred.label_ids).cpu()\n",
    "\n",
    "        # Shift logits and labels for causal language modeling.\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous().long()\n",
    "\n",
    "        # Calculate loss, ignoring masked tokens.\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "\n",
    "        # Calculate perplexity from the loss.\n",
    "        perplexity = torch.exp(loss)\n",
    "        return {\n",
    "            \"eval_loss\": loss.item(),\n",
    "            \"eval_perplexity\": perplexity.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NssrtrIDUTBu"
   },
   "source": [
    "##  Step 6: Fine-Tuning the BioGPT Model with LoRA using SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcdYFazUIbLF"
   },
   "outputs": [],
   "source": [
    "# This helper assembles individual samples into a single batch tensor.\n",
    "# It is a required component for the standard Trainer.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htKEKWPgIdoj"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 286579,
     "status": "ok",
     "timestamp": 1749937218342,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "M3sMnYNiUZTQ",
    "outputId": "92dd7614-e827-4d5d-c092-c3540d450a90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 04:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.107700</td>\n",
       "      <td>1.102924</td>\n",
       "      <td>3.049450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.926700</td>\n",
       "      <td>1.015971</td>\n",
       "      <td>2.794589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=1.4953804016113281, metrics={'train_runtime': 285.9927, 'train_samples_per_second': 4.406, 'train_steps_per_second': 0.14, 'total_flos': 1.11307687723008e+16, 'train_loss': 1.4953804016113281, 'epoch': 2.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtbh2wqAV-PD"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ynbbA0Tz7fx"
   },
   "outputs": [],
   "source": [
    "for file in SOURCE_LOG_DIR.glob(\"*\"):\n",
    "    shutil.copy(file, DEST_LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KS5-ZlojKQXV"
   },
   "source": [
    "## Step 7: Save LoRA Adapter Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9udMRORlevZ"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(ADAPTER_SAVE_PATH.as_posix())\n",
    "print(f\"LoRA adapter saved to: {ADAPTER_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5OtS1EGP-bK"
   },
   "source": [
    "## Step 8: Generate Validation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZttOhHOcAzT"
   },
   "outputs": [],
   "source": [
    "# This is crucial. We need a tokenizer instance configured for left-padding.\n",
    "# The tokenizer used for training can be different from the one for generation.\n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generation_tokenizer.padding_side = 'left'\n",
    "generation_tokenizer.pad_token = generation_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPXlQFExQBEM"
   },
   "outputs": [],
   "source": [
    "# Load validation samples\n",
    "with open(VAL_PATH, \"r\") as f:\n",
    "    samples = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SRusmv3Sw7X"
   },
   "outputs": [],
   "source": [
    "# Switch model to eval mode and disable gradients\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9KkN4FPcPQD"
   },
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "eval_samples = samples[0:10]\n",
    "results = []\n",
    "batch_size = 5\n",
    "eval_loader = DataLoader(eval_samples, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Run Batched Inference with CORRECTED LOOP LOGIC\n",
    "for batch in tqdm(eval_loader, desc=\"Generating sample outputs\"):\n",
    "\n",
    "    # The batch is a dictionary of lists. Get the number of items.\n",
    "    num_items_in_batch = len(batch['question'])\n",
    "\n",
    "    # Format prompts by iterating through the batch using an index\n",
    "    prompts = []\n",
    "    for i in range(num_items_in_batch):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"user\", \"content\": batch[\"question\"][i]}\n",
    "        ]\n",
    "        prompt_text = generation_tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        prompts.append(prompt_text)\n",
    "\n",
    "    # Tokenize with the left-padded tokenizer\n",
    "    inputs = generation_tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate with a sampling strategy to prevent loops\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        pad_token_id=generation_tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode cleanly\n",
    "    input_ids_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = output_ids[:, input_ids_len:]\n",
    "    generated_answers = generation_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Store results using the correct index-based iteration\n",
    "    for i in range(num_items_in_batch):\n",
    "        results.append({\n",
    "            \"uuid\": batch['uuid'][i],\n",
    "            \"question\": batch['question'][i],\n",
    "            \"reference_answer\": batch['answer'][i],\n",
    "            \"generated_answer\": generated_answers[i],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1749938492949,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "_3rt7KN85_TP",
    "outputId": "36af4090-2358-4b48-8335-8f6c76a8021b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uuid': 'iu_1888',\n",
       "  'question': 'What is the radiologic impression?',\n",
       "  'reference_answer': 'No acute cardiopulmonary abnormalities',\n",
       "  'generated_answer': 'Heart size and mediastinal silhouette are within normal limits. There is no acute cardiopulmonary abnormality. The aorta is tortuous, but this is within the normal range for age. The left shoulder and chest wall are normal. There is no evidence of pneumothorax or pleural effusion. No rib fractures. No acute bony abnormality. No focal airspace consolidation or effusion. No pleural thickening or pleural effusion. No pneumothorax. No rib fractures. No acute bony abnormality. No focal airspace consolidation or effusion. No pleural thickening or pleural effusion. No pneumothorax. No rib fractures. No acute bony abnormality. No focal airspace consolidation or effusion. No pleural thickening or pleural effusion. No pneumothorax. No rib fractures. No acute bony abnormality. No focal airspace consolidation or effusion. No pleural thickening or pleural effusion. No pneumothorax. No rib fractures. No acute bony abnormality. No focal airspace consolidation or effusion. No pleural thickening or pleural effusion. No pneumothorax. No rib fractures. No acute bony abnormality. No'},\n",
       " {'uuid': 'iu_1888',\n",
       "  'question': 'Summarize the key thoracic findings.',\n",
       "  'reference_answer': 'No acute cardiopulmonary abnormalities',\n",
       "  'generated_answer': 'Heart size normal. No acute cardiopulmonary abnormality. No edema. No pleural effusion or pneumothorax. No focal consolidation or infiltrate. No nodules or masses. Normal mediastinal contour. No rib fractures. No acute bony abnormalities. [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] ['},\n",
       " {'uuid': 'iu_1888',\n",
       "  'question': 'State the impression.',\n",
       "  'reference_answer': 'Normal chest X-ray; no acute cardiopulmonary issues.',\n",
       "  'generated_answer': '1. Heart size normal. 2. No acute cardiopulmonary abnormality. 3. No acute thoracic abnormality. 4. No acute abdominal abnormality. 5. No acute musculoskeletal abnormality. 6. No acute neurologic abnormality. 7. No acute vascular abnormality. 8. No acute joint or soft tissue abnormality. 9. No acute pulmonary embolism or pneumothorax. 10. No acute aortic or thoracic aortic dissection. 11. No acute vertebral body fracture. 12. No acute rib fracture. 13. No acute costophrenic thickening. 14. No acute pleural effusion. 15. No acute pleural effusion or pneumothorax. 16. No acute mediastinal contour abnormality. 17. No acute abdominal or retroperitoneal abnormality. 18. No acute vascular or musculoskeletal abnormality. 19. No acute acute or chronic changes. 20. No acute or chronic pulmonary nodularity. 21. No acute or chronic pleural effusion or pneumothorax. 22. No acute or chronic mediastinal contour abnormality. '},\n",
       " {'uuid': 'iu_5141',\n",
       "  'question': 'What is the radiologic impression?',\n",
       "  'reference_answer': 'Hyperexpanded lungs, suggesting chronic obstructive pulmonary disease. No acute pulmonary process',\n",
       "  'generated_answer': 'Heart size is normal. No acute cardiopulmonary abnormality. No pleural effusion or pneumothorax. No rib fractures. Normal aortic calcification. No acute abdominal disease. Small bowel gas. No bowel obstruction. No free air. No pneumoperitoneum. No splenic capsule rupture. No acute biliary disease. No acute renal disease. No acute thoracic aortic disease. No acute pulmonary embolus. No edema. No congestive heart failure. No pulmonary edema. No pleural effusion. No pneumothorax. No rib fractures. No acute bony abnormality. No acute musculoskeletal abnormality. No acute joint disease. No acute musculoskeletal disease. No acute abdominal disease. No acute thoracic abnormality. No acute aortic disease. No acute cardiac abnormality. No acute renal abnormality. No acute pulmonary abnormality. No acute bony abnormality. No acute musculoskeletal abnormality. No acute joint disease. No acute musculoskeletal abnormality. No acute abdominal disease. No acute thoracic abnormality. No acute aortic disease. No acute cardiac abnormality. No acute renal abnormality. No acute pulmonary abnormality. No acute bony abnormality. No'},\n",
       " {'uuid': 'iu_5141',\n",
       "  'question': 'Summarize the key thoracic findings.',\n",
       "  'reference_answer': 'Hyperexpanded lungs, suggesting chronic obstructive pulmonary disease. No acute pulmonary process',\n",
       "  'generated_answer': '1. No acute cardiopulmonary abnormality. 2. No rib fractures. 3. No pleural effusion. 4. No pneumothorax. 5. No active tuberculosis. 6. No edema. 7. No focal airspace consolidation. 8. No infiltrate. 9. No nodules. 10. No lymphadenopathy. 11. No focal alveolar edema. 12. No pleural effusion. 13. No pneumothorax. 14. No active tuberculosis. 15. No rib fractures. 16. No edema. 17. No focal airspace consolidation. 18. No infiltrate. 19. No nodules. 20. No lymphadenopathy. 21. No focal alveolar edema. 22. No pleural effusion. 23. No pneumothorax. 24. No active tuberculosis. 25. No rib fractures. 26. No edema. 27. No focal airspace consolidation. 28. No infiltrate. 29. No nodules. 30. No lymphadenopathy. 31. No focal alveolar edema. 32. No ple'},\n",
       " {'uuid': 'iu_5141',\n",
       "  'question': 'Give a brief diagnostic conclusion.',\n",
       "  'reference_answer': 'Lungs are hyperexpanded, indicating COPD; no acute pulmonary issues.',\n",
       "  'generated_answer': '1. No acute cardiopulmonary abnormality. 2. Mild degenerative changes in the spine. 3. No acute fractures. 4. No acute joint effusions. 5. No acute bony changes. 6. No acute abdominal disease. 7. No acute neurologic disease. 8. No acute vascular disease. 9. No acute musculoskeletal disease. 10. No acute chest wall disease. 11. No acute thoracic spine disease. 12. No acute lumbar spine disease. 13. No acute lower extremity disease. 14. No acute upper extremity disease. 15. No acute pelvic disease. 16. No acute peritoneal disease. 17. No acute intra-abdominal disease. 18. No acute retroperitoneal disease. 19. No acute aortic disease. 20. No acute mediastinal disease. 21. No acute left pleural effusion. 22. No acute right pleural effusion. 23. No acute effusion. 24. No acute pneumothorax. 25. No acute emphysema. 26. No acute edema. 27. No acute nodules or'},\n",
       " {'uuid': 'iu_0769',\n",
       "  'question': 'What is the radiologic impression?',\n",
       "  'reference_answer': 'Appearance suggests emphysema. Apical capping, slightly greater at the left. Recommend comparison with prior films or followup chest x-[REDACTED] films to confirm stability and approximately 4 months in [REDACTED]. Critical result notification documented through Primordial',\n",
       "  'generated_answer': 'Heart size normal. No acute cardiopulmonary abnormality. No pneumothorax. No acute thoracic trauma. No rib fractures. No acute pulmonary edema. No pleural effusion. No pneumonitis or empyema. No active tuberculosis. No mediastinal contour abnormality. No rib fractures. No acute musculoskeletal abnormality. No acute neurologic abnormality. No acute abdomen. No acute abdominal trauma. No acute abdomen. No bowel obstruction. No free air. No ascites. No pleural effusion. No pneumothorax. No acute cardiomegaly. No acute pericardial effusion. No active tuberculosis. No rib fractures. No acute musculoskeletal abnormality. No acute neurologic abnormality. No acute abdomen. No bowel obstruction. No free air. No ascites. No pleural effusion. No pneumothorax. No acute cardiomegaly. No acute pericardial effusion. No active tuberculosis. No rib fractures. No acute musculoskeletal abnormality. No acute neurologic abnormality. No acute abdomen. No bowel obstruction. No free air. No ascites. No pleural effusion. No pneumothorax. No acute cardiome'},\n",
       " {'uuid': 'iu_0769',\n",
       "  'question': 'Summarize the key thoracic findings.',\n",
       "  'reference_answer': 'Appearance suggests emphysema. Apical capping, slightly greater at the left. Recommend comparison with prior films or followup chest x-[REDACTED] films to confirm stability and approximately 4 months in [REDACTED]. Critical result notification documented through Primordial',\n",
       "  'generated_answer': 'Chest X-ray shows hyperexpanded lungs, left greater than right, suggesting emphysema or bullous disease. No pneumothorax. No focal consolidation or effusion. Mediastinal silhouette is normal. No acute cardiopulmonary abnormality. No acute abdominal disease. No acute musculoskeletal abnormality. No vertebral fracture. No acute vascular abnormality. No calcified lymph nodes. No calcified granuloma. No pneumothorax. No pleural effusion. No pleural thickening. No nodular consolidation. No atelectasis. No lobar infiltrate. No pleural effusion. No pleural thickening. No pneumothorax. No focal airspace consolidation. No pleural effusion. No pleural thickening. No nodular consolidation. No atelectasis. No acute cardiopulmonary abnormality. No acute musculoskeletal abnormality. No vertebral fracture. No acute vascular abnormality. No calcified lymph nodes. No calcified granuloma. No pneumothorax. No pleural effusion. No pleural thickening. No nodular consolidation. No atelectasis. No lobar infiltrate. No focal airspace consolidation. No pleural effusion. No'},\n",
       " {'uuid': 'iu_0769',\n",
       "  'question': 'Provide a concise report summary.',\n",
       "  'reference_answer': 'Suggestive of emphysema; apical capping, more on left. Recommend follow-up or comparison.',\n",
       "  'generated_answer': 'Normal heart size, clear lungs, no edema, no pleural effusion or pneumothorax. No acute cardiopulmonary abnormality. No focal neurologic deficit. Stable spine and no acute musculoskeletal injury. No acute abdominal disease. No evidence of metastatic disease. No acute abdominal disease. No evidence of active tuberculosis. No acute injury. No acute joint disease. No fracture. No dislocation. No acute bony abnormality. No acute vascular disease. No acute heart disease. No acute pulmonary embolus. No acute cardiomegaly. No acute pericardial effusion. No acute pleural effusion. No acute peritoneal disease. No acute intra-abdominal free air. No acute bowel obstruction. No acute diverticulitis. No acute diverticula. No acute inflammatory changes. No acute edema. No acute pleural effusion. No acute pericardial effusion. No acute pneumonia. No acute pleural effusion. No acute pneumothorax. No acute cardiomegaly. No acute heart failure. No acute pulmonary edema. No acute pleural effusion. No acute pericardial effusion. No acute peritoneal disease. No acute intra-ab'},\n",
       " {'uuid': 'iu_4308',\n",
       "  'question': 'What is the radiologic impression?',\n",
       "  'reference_answer': 'No acute cardiopulmonary process',\n",
       "  'generated_answer': '1. The lungs are clear. 2. The heart size is normal. 3. No focal pulmonary consolidation or effusion. 4. No acute cardiopulmonary abnormality. 5. No evidence of a pulmonary embolus or pneumothorax. 6. No focal airspace consolidation or pleural effusion. 7. No left atrial enlargement. 8. The aortic knob is well-defined and normal. 9. The ascending aorta and aortic arch are normal. 10. No evidence of aortic dissection. 11. No evidence of pulmonary edema. 12. No acute coronary disease. 13. No left pleural effusion or pneumothorax. 14. No acute cardiopulmonary abnormality. 15. No acute abdominal disease. 16. No bowel obstruction. 17. No pneumothorax. 18. No acute pulmonary embolus. 19. No evidence of aortic dissection. 20. No acute cardiopulmonary abnormality. 21. No acute abdominal disease. 22. No bowel obstruction. 23. No pneumothorax. 24. No acute pulmonary embolus. 25. No'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7053,
     "status": "ok",
     "timestamp": 1749938920166,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "_lwtzAHZbgMr",
    "outputId": "cd7cba8d-45a8-4c89-b13c-19bb29a9e1ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State the impression clearly in two sentences. The heart size is normal. There is no evidence of active disease or acute cardiopulmonary abnormality. The chest X-ray is otherwise unremarkable. No acute cardiopulmonary abnormality is identified. The heart size is normal. No active disease or acute cardiopulmonary abnormality is identified. The\n"
     ]
    }
   ],
   "source": [
    "prompt = \"State the impression clearly in two sentences.\"\n",
    "\n",
    "# Tokenize raw prompt only\n",
    "inputs = generation_tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate answer\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode\n",
    "decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1749939156603,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "0JJLCYWfUUsi",
    "outputId": "379973da-bb25-4113-cf4b-96a424a4cf1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation predictions to /content/drive/MyDrive/multimodal-xray-agent/data/qapairs/llama_validation_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    for example in results:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "print(f\"Saved validation predictions to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9V3bNrVZT90"
   },
   "source": [
    "## Step 9: Final Metrics + Summary Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1749939161771,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "V0Fe6XtUUrLT",
    "outputId": "69ae609b-8cdd-4854-8201-e004a8828ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-level metrics saved to: /content/drive/MyDrive/multimodal-xray-agent/logs/llama_epoch_metrics.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"epoch_logs\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6120148909539507,\n        \"min\": 0.25316455696202533,\n        \"max\": 2.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5063291139240507,\n          1.5063291139240507,\n          0.25316455696202533\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0238568479040417,\n        \"min\": 0.9264,\n        \"max\": 3.9356,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1.7588,\n          0.9264,\n          3.9356\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06148533989423861,\n        \"min\": 1.015971064567566,\n        \"max\": 1.1029244661331177,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.015971064567566,\n          1.1029244661331177\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_perplexity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18021351791160922,\n        \"min\": 2.7945892810821533,\n        \"max\": 3.0494496822357178,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2.7945892810821533,\n          3.0494496822357178\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "epoch_logs"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c73188ee-123e-4951-ab12-2f641034791b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.253165</td>\n",
       "      <td>3.9356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.506329</td>\n",
       "      <td>1.7588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.759494</td>\n",
       "      <td>1.2917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.1077</td>\n",
       "      <td>1.102924</td>\n",
       "      <td>3.049450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.253165</td>\n",
       "      <td>1.0111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.506329</td>\n",
       "      <td>0.9264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.759494</td>\n",
       "      <td>1.0050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>1.015971</td>\n",
       "      <td>2.794589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c73188ee-123e-4951-ab12-2f641034791b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c73188ee-123e-4951-ab12-2f641034791b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c73188ee-123e-4951-ab12-2f641034791b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-e487162d-9ba9-44ae-8094-44466a110628\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e487162d-9ba9-44ae-8094-44466a110628')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-e487162d-9ba9-44ae-8094-44466a110628 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_dbea4efb-02a0-418b-98cc-9614dbd4c35e\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('epoch_logs')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_dbea4efb-02a0-418b-98cc-9614dbd4c35e button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('epoch_logs');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      epoch    loss  eval_loss  eval_perplexity\n",
       "0  0.253165  3.9356        NaN              NaN\n",
       "1  0.506329  1.7588        NaN              NaN\n",
       "2  0.759494  1.2917        NaN              NaN\n",
       "3  1.000000  1.1077   1.102924         3.049450\n",
       "4  1.253165  1.0111        NaN              NaN\n",
       "5  1.506329  0.9264        NaN              NaN\n",
       "6  1.759494  1.0050        NaN              NaN\n",
       "7  2.000000  0.9267   1.015971         2.794589"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract training + eval logs (every log step)\n",
    "records = trainer.state.log_history\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Filter only epoch-level logs (those with eval_loss or epoch key)\n",
    "epoch_logs = df[df[\"epoch\"].notnull()][[\"epoch\", \"loss\", \"eval_loss\", \"eval_perplexity\"]]\n",
    "\n",
    "# Drop duplicates and keep last record per epoch (in case of multiple entries)\n",
    "epoch_logs = epoch_logs.groupby(\"epoch\").last().reset_index()\n",
    "\n",
    "# Save\n",
    "epoch_logs.to_csv(METRICS_PATH, index=False)\n",
    "\n",
    "print(f\"Epoch-level metrics saved to: {METRICS_PATH.resolve()}\")\n",
    "display(epoch_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2q95ZWKCMOs-"
   },
   "source": [
    "## Step 10: Fix Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5922,
     "status": "ok",
     "timestamp": 1749967652327,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "ym1h3VWOgIn-"
   },
   "outputs": [],
   "source": [
    "!pip install nbformat --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1749967652378,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "wNjLxJl4MZbK"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import os\n",
    "from google.colab import drive, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrLzaH90Md2w"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1066,
     "status": "ok",
     "timestamp": 1749967667303,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "DDqXzxT8Mh8d",
    "outputId": "c79cb467-19ab-4c02-af86-4cc1a94d64f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitkeep',\n",
       " '00_colab_setup.ipynb',\n",
       " '01_bootstrap.ipynb',\n",
       " '02_preprocessing.ipynb',\n",
       " '04_text_embedding_faiss_indexing.ipynb',\n",
       " '03_image_embedding_faiss_indexing.ipynb',\n",
       " '05_iu_xray_processing.ipynb',\n",
       " '06_generate_qa_pairs.ipynb',\n",
       " '08_finetune_biogpt_lora_run2.ipynb',\n",
       " '10_tokenization.ipynb',\n",
       " '09_llama3_zero_shot_eval.ipynb',\n",
       " '07_finetune_biogpt_lora.ipynb',\n",
       " 'Copy of 10_tokenization.ipynb',\n",
       " '10_tokenization_fixed.ipynb',\n",
       " '12_llama3_finetuned_eval.ipynb',\n",
       " '11_finetune_llama3.2_lora.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the notebook directory to confirm the file exists\n",
    "os.listdir(\"/content/drive/MyDrive/multimodal-xray-agent/notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQtE5gY3Mlcr"
   },
   "outputs": [],
   "source": [
    "notebook_path = \"/content/drive/MyDrive/multimodal-xray-agent/notebooks/11_finetune_llama3.2_lora.ipynb\"\n",
    "\n",
    "with open(notebook_path, \"r\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "with open(notebook_path, \"w\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Notebook fixed and saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP9sV0+fvJrWxbHtODjCr+2",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
